# jemdoc: menu{MENU}{index.html}
# jemdoc: analytics{UA-106338466-1}
#= jemdoc -- light markup
= home

# {{</br>}}
# {{</br>}}
# I am an assistant professor at Computer Science Department at the University of Arizona.
# My research focuses on sequential decision-making in feedback loops (i.e., the multi-armed bandit problem) and online learning.
# I also had some fun in the past with machine learning applied to psychology.  
# 
# - [https://scholar.google.com/citations?user=VgvC7o8AAAAJ&hl=en Google scholar], [./data/kjun-cv.pdf CV]
# - E-mail: (kjun å† cs ∂ø† arizona ∂ø† edu), 
# - Address: 746 Gould-Simpson, 1040 E. 4th St., Tucson, AZ 85721, USA.
# 
~~~
{}{img_left}{./imgs/kwang-sung_jun_pic_2.jpg}{profile_pic}{240px}{HEIGHTpx}

Assistant Professsor{{</br>}}
Computer Science{{</br>}}
Statistics GIDP, Applied Math GIDP (affiliated){{</br>}}
University of Arizona{{</br>}}

kjun å† cs ∂ø† arizona ∂ø† edu{{</br>}}
Gould-Simpson Rm 746, 1040 E. 4th St., Tucson, AZ 85721 {{</br>}}
[https://scholar.google.com/citations?user=VgvC7o8AAAAJ&hl=en Google scholar]{{</br>}}
[./data/kjun-cv.pdf CV]

~~~

# {{<font color=D95B43><b> 
# (Sep'21) I am actively recruiting PhD students.</b> </br>
# Required background is probability, statistics, linear algebra, calculus, and some CS backgrounds. 
# Background in ML theory is a plus. Your undergrad major does not matter, but minimal CS background is required (we can discuss it further). For this position, you should expect to spend a lot of time on mathematical research (e.g., algorithms with performance guarantees) as opposed to tuning hyperparameters on benchmark datasets. This, however, does not mean we will only stick to theories. We will develop performant algorithms and test them in real-world tasks. You can find my email from my webpage and send me an email if you are interested.
# </font>}}

== intro

Broadly, I work on interactive machine learning.
I spend most of my time on on developing and analyzing adaptive decision-making\/sampling methods including bandit algorithms and reinforcement learning.
I tend to revolve around simple problems. 
Recently, I am also looking into Monte Carlo tree search methods and various applications including efficient matrix decomposition, geoscience (some blackbox\/bayesian optimization involved), and material science problems.
#My research is centered around sequential decision-making in feedback loops (i.e., the multi-armed bandit problem) and online (machine, not human) learning.
I also had some fun in the past with machine learning applied to psychology.  
I was previously a postdoc with [http://francesco.orabona.com/ Francesco Orabona] (who I call the 'master of coin') at Boston University.
Before then, I spent 9 years at UW-Madison for a PhD degree with [http://pages.cs.wisc.edu/~jerryzhu/ Xiaojin (Jerry) Zhu] and a postdoc position at [https://discovery.wisc.edu/ Wisconsin Institute for Discovery] with [http://nowak.ece.wisc.edu Robert Nowak], [http://willett.ece.wisc.edu/ Rebecca Willett], and [http://pages.cs.wisc.edu/~swright/ Stephen Wright].


== news

- 09\/22: 2 papers accepted at NeurIPS. Congrats to my postdoc Kyoungseok Jang!
- 05\/22: I gave a talk at the [https://sites.google.com/view/rltheoryseminars/home RL theory virtual seminars] on Maillard sampling.
- 01\/22: 1 paper accpeted at AAAI, 3 papers accepted to AISTATS.
- 05\/21: 2 papers accepted at ICML. 
- 05\/21: 1 paper accepted at ISIT. 
- 07\/20: I gave a talk at the [https://sites.google.com/view/rltheoryseminars/home RL theory virtual seminars] on structured bandits with the asymptotic optimality.
- 07\/20: Chicheng and I have a new work on structured bandits that is accepted to ICML'20 workshop on theoretical foundations of reinforcement learning!
- 11\/19: In Spring 2020, I will be teaching [./20.1.csc665/index.html CSC 665 Online Learning and Multi-armed Bandits].
- 10\/19: Chicheng Zhang, Jason Pacheco, and I are organizing a [https://sites.google.com/view/ua-mlrg/ machine learning reading group] at UA.
- 10\/19: Our paper on kernel regression paper is accepted at NeurIPS'19.
- 10\/19: Our paper on parameter-free SGD with local differential privacy is accepted to PriML'19 (worshop at NeurIPS).

== multi-armed bandits

Multi-armed bandit is a /state-less/ version of the reinforcement learning (RL).
But why study an easier version of RL while RL seems to be solving all the problems these day?
Bandits are simpler and thus have strong theoretical guarantees and yet have abundant real-world applications.
Furthermore, developments in bandits can potentially improve RL algorithms, either transferring ideas from bandits to RL or directly use bandits for Monte Carlo planning in MDP (e.g., the Monte Carlo tree search algorithm used in AlphaGo was originated from [https://dl.acm.org/doi/10.1007/11871842_29 this paper]).

Informally speaking, bandits learn to make better decisions over time in a feedback-loop.
The decisions necessarily affect the feedback information, and the feedback data collected so far is no longer i.i.d.; most traditional learning guarantees do not apply.

Bandits are actively being studied in both [https://tor-lattimore.com/downloads/book/book.pdf theory] and [http://rob.schapire.net/papers/www10.pdf applications] including [https://mwtds.azurewebsites.net/ deployable web service] and [https://arxiv.org/abs/1603.06560 hyperparameter optimization] (check [https://docs.ray.io/en/master/tune/api_docs/schedulers.html\#tune-scheduler-hyperband ray implementation]).
Also, the cartoon caption contest of New Yorker is using bandit algorithms to efficiently crowdsource caption evaluations ([http://nextml.org/2015/12/04/new-yorker.html this article])!


== talks

- 07\/20: At [https://sites.google.com/view/rltheoryseminars/home RL theory virtual seminars], "Crush Optimism with Pessimism: Structured Bandits Beyond Asymptotic Optimality." [https://www.youtube.com/watch?v=S8dluBz4lMU \[video\]]
- 05\/20: At Los Alamos - Arizona Days Conference, "Adaptive data collection for accelerating discovery rates."
- 09\/19: At TRIPODS seminar, the U of Arizona, "Adaptive data collection for accelerating discovery rates."  
- 09\/19: At TRIPODS RWG6, the U of Arizona, "Accelerating discovery rate in adaptive experiments via bandits with low-rank structure." 
- 07\/19: At Microsoft New England, "Accelerating discovery rate in adaptive experiments via bandits with low-rank structure."
- 04\/19: At the U of Arizona, "Accelerating discovery rate in adaptive experiments via bandits with low-rank structure."
- 10\/18: At Open AIR: Industry Open House, Boston University, "Adapting to changing environments in online learning."
- 10\/17: At [http://silo.ece.wisc.edu/web/ SILO], "Scalable Generalized Linear Bandits: Online Computation and Hashing." [http://silo.ece.wisc.edu/web/content/seminar/id/243 \[abstract\]]
- 04\/17: At [http://www.aistats.org/aistats2017//schedule.html AISTATS], "Improved Strongly Adaptive Online Learning using Coin Betting."
- 06\/16: At [http://cpcp.wisc.edu/ CPCP] Annual Retreat, "Multi-Armed Bandit Algorithms and Applications to Experiment Selection." [http://cpcp.wisc.edu/resources/cpcp-retreat-2016-multi-armed-bandit-algorithms-and-applications-to-experiment-selection-kwang-sung-jun-6-30-2016 \[abstract & video\]]
- 03\/16: At [http://silo.ece.wisc.edu/web/ SILO], "Top Arm Identification in Multi-Armed Bandits with Batch Arm Pulls." [http://silo.ece.wisc.edu/web/content/seminar/id/198 \[abstract & video\]]
- 03\/16: At [http://eng.ssu.ac.kr/web/eng/edu_a_01_07_a Soongsil University], two talks on human memory search.
- 06\/16: At [http://icml.cc/2016/ ICML], "Anytime Exploration for Multi-armed Bandits using Confidence Information." [http://techtalks.tv/talks/anytime-exploration-for-multi-armed-bandits-using-confidence-information/62425/ \[video\]]
- 11\/15: At HAMLET (interdisciplinary seminar series at UW-Madison), "Measuring semantic structure from verbal fluency data with the initial-visit-emitting (INVITE) random walk."
- 03\/15: At [http://www.ttic.edu/ TTIC], "Learning from Human-Generated Lists."
- 06\/13: At [http://icml.cc/2013/?page_id=47 ICML], "Learning from Human-Generated Lists." [http://techtalks.tv/talks/learning-from-human-generated-lists/58261/ \[video\]]


# == services
# 
# - Program Committee / Reviewer: AISTATS'20 reviewer, AAAI'20 Area Chair, ICML'19, COLT'19 (subreviewer), IJCAI'19, NeurIPS'19, IEEE Transactions on Signal Processing, ICML'19, AISTATS'19 ,NeurIPS'18, AISTATS'18, AAAI'18, NeurIPS'17, ICML'17, COLT'17 (subreviewer), AISTATS'17, ICML'16.


# # jemdoc: menu{MENU}{index.html}, showsource
# = jemdoc -- light markup
# [https://jemnz.com/ Jacob Mattingley] ([www@jemnz.com])
# 
# jemdoc is a light text-based markup language designed for creating 
# websites. It takes a text file written with [example.html jemdoc markup], an
# optional configuration file and an optional menu file, and makes static websites
# that look something like this one, [https://jemnz.com that one] or
# [http://www.stanford.edu/class/ee364a/ another one].
# 
# jemdoc was inspired by [http://www.methods.co.nz/asciidoc/ AsciiDoc], which is a
# text document format. AsciiDoc is great, and lots of the ideas from AsciiDoc are
# copied in jemdoc. The main differences are that jemdoc is simpler (you could say
# deliberately feature poor) and has more consistent syntax.
# 
# [download.html Download jemdoc.]
# 
# ~~~
# Version +0.7.3+ was released on 2012-11-27 with some small bug fixes.
# See the [revision.html release notes].
# ~~~
# 
# [https://github.com/jem/jemdoc Contribute to jemdoc on github.]
# 
# == Goals
# - Simple, consistent syntax.
# - $\mbox{\LaTeX}$ [latex.html equation support].
# - [tables.html Table support].
# - Portability. The (single) jemdoc [http://www.python.org/ Python] file \+
#   single css file \+ your input file {{&rarr;}} html.
# - Based on [http://www.w3.org/Style/CSS/ CSS] so formatting specifics are
#   independent of jemdoc.
# - Production of clean,
#   [http://validator.w3.org/check/referer
#   standards-compliant] [http://www.w3.org/TR/xhtml11/ XHTML 1.1].
# - Minimal bells and whistles, but simple fallback to raw html if required.
# 
# == License
# Copyright \C 2007--2012 Jacob Mattingley.
# 
# jemdoc is free software; you can redistribute it and\/or modify it under the
# terms of the [http://www.gnu.org/licenses/gpl-3.0.html GNU General Public
# License] as published by the Free Software Foundation; either version 3 of the
# License, or (at your option) any later version.
# 
# jemdoc is distributed in the hope that it will be useful, but WITHOUT ANY
# WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A
# PARTICULAR PURPOSE. See the [http://www.gnu.org/licenses/gpl-3.0.html GNU
# General Public License] for more details.
