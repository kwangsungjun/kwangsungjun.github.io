# jemdoc: menu{MENU}{index.html}
# jemdoc: analytics{UA-106338466-1}
= Home

# {{</br>}}
# {{</br>}}
# I am an assistant professor at Computer Science Department at the University of Arizona.
# My research focuses on sequential decision-making in feedback loops (i.e., the multi-armed bandit problem) and online learning.
# I also had some fun in the past with machine learning applied to psychology.  
# 
# - [https://scholar.google.com/citations?user=VgvC7o8AAAAJ&hl=en Google scholar], [./data/kjun-cv.pdf CV]
# - E-mail: (kjun å† cs ∂ø† arizona ∂ø† edu), 
# - Address: 746 Gould-Simpson, 1040 E. 4th St., Tucson, AZ 85721, USA.
# 
~~~
{}{img_left}{./imgs/2024 - dsc08267 resized v3.jpg}{profile_pic}{280px}{HEIGHTpx}

Kwang-Sung Jun{{</br>}}
Assistant Professsor{{</br>}}
Computer Science{{</br>}}
Statistics GIDP, Applied Math GIDP (affiliated){{</br>}}
University of Arizona{{</br>}}

kjun å† cs ∂ø† arizona ∂ø† edu{{</br>}}
Gould-Simpson Rm 746, 1040 E. 4th St., Tucson, AZ 85721 {{</br>}}
[https://scholar.google.com/citations?user=VgvC7o8AAAAJ&hl=en Google scholar]{{</br>}}
[./data/kjun-cv.pdf CV]

~~~

# {{<font color=D95B43><b> 
# (Sep'21) I am actively recruiting PhD students.</b> </br>
# Required background is probability, statistics, linear algebra, calculus, and some CS backgrounds. 
# Background in ML theory is a plus. Your undergrad major does not matter, but minimal CS background is required (we can discuss it further). For this position, you should expect to spend a lot of time on mathematical research (e.g., algorithms with performance guarantees) as opposed to tuning hyperparameters on benchmark datasets. This, however, does not mean we will only stick to theories. We will develop performant algorithms and test them in real-world tasks. You can find my email from my webpage and send me an email if you are interested.
# </font>}}

== Intro

Broadly, I am interested in interactive machine learning (IML), which includes reinforcement learning, bandits, Bayesian optimization, and active learning (see below for some introduction of these). I also develop novel confidence bounds that often becomes key tools for constructing efficient IML algorithms. Recently, I have been studying IML problems arising from GenAIs including LLMs.

Before coming to UA, I did a postdoc with [http://francesco.orabona.com/ Francesco Orabona] (who I call the 'master of coin') at Boston University.
Before then, I spent 9 years at UW-Madison for a PhD degree with [http://pages.cs.wisc.edu/~jerryzhu/ Xiaojin (Jerry) Zhu] and a postdoc position at [https://discovery.wisc.edu/ Wisconsin Institute for Discovery] with [http://nowak.ece.wisc.edu Robert Nowak], [https://willett.psd.uchicago.edu/ Rebecca Willett], and [http://pages.cs.wisc.edu/~swright/ Stephen Wright].

# I spend most of my time on on developing and analyzing adaptive decision-making\/sampling methods including bandit algorithms and reinforcement learning.
# I tend to revolve around simple problems. 
  # Recently, I am also looking into Monte Carlo tree search methods and various applications including efficient matrix decomposition, geoscience (some blackbox\/bayesian optimization involved), and material science problems.
  #My research is centered around sequential decision-making in feedback loops (i.e., the multi-armed bandit problem) and online (machine, not human) learning.
  #I also had some fun in the past with machine learning applied to psychology.  

== News

- Nov'24: I gave a talk at UW-Madison SILO on 'Confidence Sequences via Online Learning' ([https://silo.wisc.edu/talk/09122024-2-2-2-2-2/ video link]). 
- Oct'24: 2 papers accepted to NeurIPS: (i) led by Yao, bandit experiments with instrumental variables, so you can experiment when you can't experiment! (ii) led by Junghyun, unified confidence sets for generalized linear models and finally removing the norm dependence in logistic bandits with novel regret analysis!
- May'24: 2 papers accepted to ICML: (i) adapting to the unknown noise level in linear bandits (ii) low-rank matrix recovery with better guarantees than the nuclear-norm regularization, how we can perform design of experiments for better subspace recovery, and how to solve low-rank bandits with them!
- May'24: 1 paper accepted to COLT on PAC-Bayes bounds with a different divergence that is better than KL!
- Feb'24: I will serve as an action editor for the journal /Machine Learning/.
- Jan'24: 1 paper accepted to AISTATS on converting a regret bound into a tight confidence set. Congrats Junghyun!
- Dec'23: 1 paper accepted to NeurIPS. Congrats Hao Qin!
# - Jul'23: 1 paper accepted to COLT on a very tight PAC-Bayes bound!
# - Jul'23: 1 paper accepted to ICML. Congrats to my student Yao and my collaborators Connor and Csaba!
# - Sep'22: 2 papers accepted at NeurIPS. Congrats to my postdoc Kyoungseok Jang!
# - May'22: I gave a talk at the [https://sites.google.com/view/rltheoryseminars/home RL theory virtual seminars] on Maillard sampling.
# - Jan'22: 1 paper accpeted at AAAI, 3 papers accepted to AISTATS.
# - May'21: 2 papers accepted at ICML. 
# - May'21: 1 paper accepted at ISIT. 
# - Jul'20: I gave a talk at the [https://sites.google.com/view/rltheoryseminars/home RL theory virtual seminars] on structured bandits with the asymptotic optimality.
# - Jul'20: Chicheng and I have a new work on structured bandits that is accepted to ICML'20 workshop on theoretical foundations of reinforcement learning!
# - Nov'19: In Spring 2020, I will be teaching [./20.1.csc665/index.html CSC 665 Online Learning and Multi-armed Bandits].
# - Oct'19: Chicheng Zhang, Jason Pacheco, and I are organizing a [https://sites.google.com/view/ua-mlrg/ machine learning reading group] at UA.
# - Oct'19: Our paper on kernel regression paper is accepted at NeurIPS'19.
# - Oct'19: Our paper on parameter-free SGD with local differential privacy is accepted to PriML'19 (worshop at NeurIPS).


== Interactive machine learning

IML is an umbrella term for ML problems where the agent's decision is an *action* that affects which data\/feedback s\/he gets to receive as opposed to *prediction* that does not.
For example, recommending a product is an *action* since we get click feedback for the recommended product, not for the unrecommended products (and we don't know what would have happened if we recommended another item). 
Forecasting tomorrow's weather is a *prediction* since the weather tomorrow is not affected by which forecast we have made.
Basically, if you are getting feedback on GenAI models, then the right approach is to view the generated outputs as 'action' and leverage tools from interactive machine learning!
# I tend to revolve around simple problems.

== Bandit problems

The multi-armed bandit problem is a /state-less/ version of reinforcement learning (RL).
Informally speaking, bandit algorithms learn to make better decisions over time in a feedback-loop.
The decisions necessarily affect the feedback information, and the feedback data collected so far is no longer i.i.d.; most traditional learning guarantees do not apply.
But why study an easier version of RL while RL seems to be solving all the problems these day?

 - Being a very simple problem, you can develop algorithms with precise theoretical guarantees and superior performance compared to RL algorithms applied to bandit problems. These guarantees include precise instance-dependent guarantees (as opposed to the worst-case or minimax guarantees) where some algorithms even achieve optimal rates with exact numerical constants!
 - Bayesian optimization's convergence guarantees are analyzed in the bandit setup.
 - Developments in bandits are being transferred to propose new RL algorithms with strong guarantees.
 - Monte Carlo tree search (MCTS) algorithm used in AlphaGo was originated from the paper [https://dl.acm.org/doi/10.1007/11871842_29 "bandit based Monte-Carlo planning"], and MCTS made a revolutionary performance improvement in solving Go since its appearance. UCT was extended to PUCT and used in AlphaGo and numerous other successful RL applications from DeepMind and other applications (e.g., [https://www.nature.com/articles/nature25978 chemical synthesis]).
 - Bandit algorithms were used to improve the computational complexity of k-medoids problem (similar to k-Means) dramatically; e.g., [https://proceedings.neurips.cc/paper/2019/file/c4de8ced6214345614d33fb0b16a8acd-Paper.pdf this paper].

Bandits are actively being studied in both [https://tor-lattimore.com/downloads/book/book.pdf theory] and [http://rob.schapire.net/papers/www10.pdf applications] including [https://mwtds.azurewebsites.net/ deployable web service] and [https://arxiv.org/abs/1603.06560 hyperparameter optimization] (check [https://docs.ray.io/en/master/tune/api_docs/schedulers.html\#tune-scheduler-hyperband ray implementation]).
Also, the cartoon caption contest of New Yorker is using bandit algorithms to efficiently crowdsource caption evaluations ([http://nextml.org/2015/12/04/new-yorker.html this article])!

== Service

- Area chair: COLT, ALT, NeurIPS, AAAI, ICML
- Program committee: AISTATS
- Action editor for /Machine Learning/.

# # jemdoc: menu{MENU}{index.html}, showsource
# = jemdoc -- light markup
# [https://jemnz.com/ Jacob Mattingley] ([www@jemnz.com])
# 
# jemdoc is a light text-based markup language designed for creating 
# websites. It takes a text file written with [example.html jemdoc markup], an
# optional configuration file and an optional menu file, and makes static websites
# that look something like this one, [https://jemnz.com that one] or
# [http://www.stanford.edu/class/ee364a/ another one].
# 
# jemdoc was inspired by [http://www.methods.co.nz/asciidoc/ AsciiDoc], which is a
# text document format. AsciiDoc is great, and lots of the ideas from AsciiDoc are
# copied in jemdoc. The main differences are that jemdoc is simpler (you could say
# deliberately feature poor) and has more consistent syntax.
# 
# [download.html Download jemdoc.]
# 
# ~~~
# Version +0.7.3+ was released on 2012-11-27 with some small bug fixes.
# See the [revision.html release notes].
# ~~~
# 
# [https://github.com/jem/jemdoc Contribute to jemdoc on github.]
# 
# == Goals
# - Simple, consistent syntax.
# - $\mbox{\LaTeX}$ [latex.html equation support].
# - [tables.html Table support].
# - Portability. The (single) jemdoc [http://www.python.org/ Python] file \+
#   single css file \+ your input file {{&rarr;}} html.
# - Based on [http://www.w3.org/Style/CSS/ CSS] so formatting specifics are
#   independent of jemdoc.
# - Production of clean,
#   [http://validator.w3.org/check/referer
#   standards-compliant] [http://www.w3.org/TR/xhtml11/ XHTML 1.1].
# - Minimal bells and whistles, but simple fallback to raw html if required.
# 
# == License
# Copyright \C 2007--2012 Jacob Mattingley.
# 
# jemdoc is free software; you can redistribute it and\/or modify it under the
# terms of the [http://www.gnu.org/licenses/gpl-3.0.html GNU General Public
# License] as published by the Free Software Foundation; either version 3 of the
# License, or (at your option) any later version.
# 
# jemdoc is distributed in the hope that it will be useful, but WITHOUT ANY
# WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A
# PARTICULAR PURPOSE. See the [http://www.gnu.org/licenses/gpl-3.0.html GNU
# General Public License] for more details.
