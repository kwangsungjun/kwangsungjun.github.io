<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Home</title>
</head>
<body>
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
    var pageTracker = _gat._getTracker("UA-106338466-1");
    pageTracker._trackPageview();
} catch(err) {}</script>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Kwang-Sung Jun</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="publication.html">Publications</a></div>
<div class="menu-item"><a href="talk.html">Talks</a></div>
<div class="menu-item"><a href="artifact.html">Artifacts</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
<div class="menu-item"><a href="personal.html">Personal&nbsp;stuff</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Home</h1>
</div>
<table class="imgtable"><tr><td>
<img src="./imgs/kwang-sung_jun_pic_2.jpg" alt="profile_pic" width="240px" height="HEIGHTpx" />&nbsp;</td>
<td align="left"><p>Kwang-Sung Jun</br>
Assistant Professsor</br>
Computer Science</br>
Statistics GIDP, Applied Math GIDP (affiliated)</br>
University of Arizona</br>
</p>
<p>kjun å† cs ∂ø† arizona ∂ø† edu</br>
Gould-Simpson Rm 746, 1040 E. 4th St., Tucson, AZ 85721 </br>
<a href="https://scholar.google.com/citations?user=VgvC7o8AAAAJ&amp;hl=en" target=&ldquo;blank&rdquo;>Google scholar</a></br>
<a href="./data/kjun-cv.pdf" target=&ldquo;blank&rdquo;>CV</a>
</p>
</td></tr></table>
<h2>Intro</h2>
<p>Broadly, I work on interactive machine learning.
I spend most of my time on on developing and analyzing adaptive decision-making/sampling methods including bandit algorithms and reinforcement learning.
I tend to revolve around simple problems. 
Recently, I am also looking into Monte Carlo tree search methods and various applications including efficient matrix decomposition, geoscience (some blackbox/bayesian optimization involved), and material science problems.

I also had some fun in the past with machine learning applied to psychology.  
I was previously a postdoc with <a href="http://francesco.orabona.com/" target=&ldquo;blank&rdquo;>Francesco Orabona</a> (who I call the 'master of coin&rsquo;) at Boston University.
Before then, I spent 9 years at UW-Madison for a PhD degree with <a href="http://pages.cs.wisc.edu/~jerryzhu/" target=&ldquo;blank&rdquo;>Xiaojin (Jerry) Zhu</a> and a postdoc position at <a href="https://discovery.wisc.edu/" target=&ldquo;blank&rdquo;>Wisconsin Institute for Discovery</a> with <a href="http://nowak.ece.wisc.edu" target=&ldquo;blank&rdquo;>Robert Nowak</a>, <a href="http://willett.ece.wisc.edu/" target=&ldquo;blank&rdquo;>Rebecca Willett</a>, and <a href="http://pages.cs.wisc.edu/~swright/" target=&ldquo;blank&rdquo;>Stephen Wright</a>.
</p>
<h2>News</h2>
<ul>
<li><p>Jul&rsquo;23: 1 paper accepted to COLT on a very tight PAC-Bayes bound!
</p>
</li>
<li><p>Jul&rsquo;23: 1 paper accepted to ICML. Congrats to my student Yao and my collaborators Connor and Csaba!
</p>
</li>
<li><p>Sep&rsquo;22: 2 papers accepted at NeurIPS. Congrats to my postdoc Kyoungseok Jang!
</p>
</li>
<li><p>May&rsquo;22: I gave a talk at the <a href="https://sites.google.com/view/rltheoryseminars/home" target=&ldquo;blank&rdquo;>RL theory virtual seminars</a> on Maillard sampling.
</p>
</li>
<li><p>Jan&rsquo;22: 1 paper accpeted at AAAI, 3 papers accepted to AISTATS.








</p>
</li>
</ul>
<h2>Multi-armed bandits</h2>
<p>The multi-armed bandit problem is a <i>state-less</i> version of reinforcement learning (RL).
Informally speaking, bandit algorithms learn to make better decisions over time in a feedback-loop.
The decisions necessarily affect the feedback information, and the feedback data collected so far is no longer i.i.d.; most traditional learning guarantees do not apply.
But why study an easier version of RL while RL seems to be solving all the problems these day?
</p>
<ul>
<li><p>Being a very simple problem, you can develop algorithms with precise theoretical guarantees and superior performance compared to RL algorithms applied to bandit problems. These guarantees include precise instance-dependent guarantees (as opposed to the worst-case or minimax guarantees) where some algorithms even achieve optimal rates with exact numerical constants!
</p>
</li>
<li><p>Bayesian optimization's convergence guarantees are analyzed in the bandit setup.
</p>
</li>
<li><p>Developments in bandits are being transferred to propose new RL algorithms with strong guarantees.
</p>
</li>
<li><p>Monte Carlo tree search (MCTS) algorithm used in AlphaGo was originated from the paper <a href="https://dl.acm.org/doi/10.1007/11871842_29" target=&ldquo;blank&rdquo;>&ldquo;bandit based Monte-Carlo planning&rdquo;</a>, and MCTS made a revolutionary performance improvement in solving Go since its appearance. UCT was extended to PUCT and used in AlphaGo and numerous other successful RL applications from DeepMind and other applications (e.g., <a href="https://www.nature.com/articles/nature25978" target=&ldquo;blank&rdquo;>chemical synthesis</a>).
</p>
</li>
<li><p>Bandit algorithms were used to improve the computational complexity of k-medoids problem (similar to k-Means) dramatically; e.g., <a href="https://proceedings.neurips.cc/paper/2019/file/c4de8ced6214345614d33fb0b16a8acd-Paper.pdf" target=&ldquo;blank&rdquo;>this paper</a>.
</p>
</li>
</ul>
<p>Bandits are actively being studied in both <a href="https://tor-lattimore.com/downloads/book/book.pdf" target=&ldquo;blank&rdquo;>theory</a> and <a href="http://rob.schapire.net/papers/www10.pdf" target=&ldquo;blank&rdquo;>applications</a> including <a href="https://mwtds.azurewebsites.net/" target=&ldquo;blank&rdquo;>deployable web service</a> and <a href="https://arxiv.org/abs/1603.06560" target=&ldquo;blank&rdquo;>hyperparameter optimization</a> (check <a href="https://docs.ray.io/en/master/tune/api_docs/schedulers.html#tune-scheduler-hyperband" target=&ldquo;blank&rdquo;>ray implementation</a>).
Also, the cartoon caption contest of New Yorker is using bandit algorithms to efficiently crowdsource caption evaluations (<a href="http://nextml.org/2015/12/04/new-yorker.html" target=&ldquo;blank&rdquo;>this article</a>)!
</p>
<h2>Service</h2>
<ul>
<li><p>Area chair: COLT, ALT, NeurIPS, AAAI
</p>
</li>
<li><p>Program committee: ICML, AISTATS
</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2024-01-23 19:33:44 MST, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
