<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Home</title>
</head>
<body>
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
    var pageTracker = _gat._getTracker("UA-106338466-1");
    pageTracker._trackPageview();
} catch(err) {}</script>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Kwang-Sung Jun</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="publication.html">Publications</a></div>
<div class="menu-item"><a href="research-group.html">Research&nbsp;Group</a></div>
<div class="menu-item"><a href="talk.html">Talks</a></div>
<div class="menu-item"><a href="artifact.html">Artifacts</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
<div class="menu-item"><a href="personal.html">Personal&nbsp;Stuff</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Home</h1>
</div>
<table class="imgtable"><tr><td>
<img src="./imgs/2024 - dsc08267 resized v3.jpg" alt="profile_pic" width="280px" height="HEIGHTpx" />&nbsp;</td>
<td align="left"><p>Kwang-Sung Jun</br>
Assistant Professsor</br>
Computer Science</br>
Statistics GIDP, Applied Math GIDP (affiliated)</br>
University of Arizona</br>
</p>
<p>kjun å† cs ∂ø† arizona ∂ø† edu</br>
Gould-Simpson Rm 746, 1040 E. 4th St., Tucson, AZ 85721 </br>
<a href="https://scholar.google.com/citations?user=VgvC7o8AAAAJ&amp;hl=en" target=&ldquo;blank&rdquo;>Google scholar</a></br>
<a href="./data/kjun-cv.pdf" target=&ldquo;blank&rdquo;>CV</a>
</p>
</td></tr></table>
<h2>Intro</h2>
<p>Broadly, I am interested in interactive machine learning (IML), which includes reinforcement learning, bandits, Bayesian optimization, and active learning (see below for some introduction of these). I also develop novel confidence bounds that often becomes key tools for constructing efficient IML algorithms. Recently, I have been studying IML problems arising from GenAIs including LLMs.
</p>
<p>Before coming to UA, I did a postdoc with <a href="http://francesco.orabona.com/" target=&ldquo;blank&rdquo;>Francesco Orabona</a> (who I call the 'master of coin&rsquo;) at Boston University.
Before then, I spent 9 years at UW-Madison for a PhD degree with <a href="http://pages.cs.wisc.edu/~jerryzhu/" target=&ldquo;blank&rdquo;>Xiaojin (Jerry) Zhu</a> and a postdoc position at <a href="https://discovery.wisc.edu/" target=&ldquo;blank&rdquo;>Wisconsin Institute for Discovery</a> with <a href="http://nowak.ece.wisc.edu" target=&ldquo;blank&rdquo;>Robert Nowak</a>, <a href="https://willett.psd.uchicago.edu/" target=&ldquo;blank&rdquo;>Rebecca Willett</a>, and <a href="http://pages.cs.wisc.edu/~swright/" target=&ldquo;blank&rdquo;>Stephen Wright</a>.
</p>
<h2>News</h2>
<ul>
<li><p>Nov&rsquo;24: I gave a talk at UW-Madison SILO on 'Confidence Sequences via Online Learning&rsquo; (<a href="https://silo.wisc.edu/talk/09122024-2-2-2-2-2/" target=&ldquo;blank&rdquo;>video link</a>). 
</p>
</li>
<li><p>Oct&rsquo;24: 2 papers accepted to NeurIPS: (i) led by Yao, bandit experiments with instrumental variables, so you can experiment when you can't experiment! (ii) led by Junghyun, unified confidence sets for generalized linear models and finally removing the norm dependence in logistic bandits with novel regret analysis!
</p>
</li>
<li><p>May&rsquo;24: 2 papers accepted to ICML: (i) adapting to the unknown noise level in linear bandits (ii) low-rank matrix recovery with better guarantees than the nuclear-norm regularization, how we can perform design of experiments for better subspace recovery, and how to solve low-rank bandits with them!
</p>
</li>
<li><p>May&rsquo;24: 1 paper accepted to COLT on PAC-Bayes bounds with a different divergence that is better than KL!
</p>
</li>
<li><p>Feb&rsquo;24: I will serve as an action editor for the journal <i>Machine Learning</i>.
</p>
</li>
<li><p>Jan&rsquo;24: 1 paper accepted to AISTATS on converting a regret bound into a tight confidence set. Congrats Junghyun!
</p>
</li>
<li><p>Dec&rsquo;23: 1 paper accepted to NeurIPS. Congrats Hao Qin!













</p>
</li>
</ul>
<h2>Interactive machine learning</h2>
<p>IML is an umbrella term for ML problems where the agent's decision is an <b>action</b> that affects which data/feedback s/he gets to receive as opposed to <b>prediction</b> that does not.
For example, recommending a product is an <b>action</b> since we get click feedback for the recommended product, not for the unrecommended products (and we don't know what would have happened if we recommended another item). 
Forecasting tomorrow's weather is a <b>prediction</b> since the weather tomorrow is not affected by which forecast we have made.
Basically, if you are getting feedback on GenAI models, then the right approach is to view the generated outputs as 'action&rsquo; and leverage tools from interactive machine learning!

</p>
<h2>Bandit problems</h2>
<p>The multi-armed bandit problem is a <i>state-less</i> version of reinforcement learning (RL).
Informally speaking, bandit algorithms learn to make better decisions over time in a feedback-loop.
The decisions necessarily affect the feedback information, and the feedback data collected so far is no longer i.i.d.; most traditional learning guarantees do not apply.
But why study an easier version of RL while RL seems to be solving all the problems these day?
</p>
<ul>
<li><p>Being a very simple problem, you can develop algorithms with precise theoretical guarantees and superior performance compared to RL algorithms applied to bandit problems. These guarantees include precise instance-dependent guarantees (as opposed to the worst-case or minimax guarantees) where some algorithms even achieve optimal rates with exact numerical constants!
</p>
</li>
<li><p>Bayesian optimization's convergence guarantees are analyzed in the bandit setup.
</p>
</li>
<li><p>Developments in bandits are being transferred to propose new RL algorithms with strong guarantees.
</p>
</li>
<li><p>Monte Carlo tree search (MCTS) algorithm used in AlphaGo was originated from the paper <a href="https://dl.acm.org/doi/10.1007/11871842_29" target=&ldquo;blank&rdquo;>&ldquo;bandit based Monte-Carlo planning&rdquo;</a>, and MCTS made a revolutionary performance improvement in solving Go since its appearance. UCT was extended to PUCT and used in AlphaGo and numerous other successful RL applications from DeepMind and other applications (e.g., <a href="https://www.nature.com/articles/nature25978" target=&ldquo;blank&rdquo;>chemical synthesis</a>).
</p>
</li>
<li><p>Bandit algorithms were used to improve the computational complexity of k-medoids problem (similar to k-Means) dramatically; e.g., <a href="https://proceedings.neurips.cc/paper/2019/file/c4de8ced6214345614d33fb0b16a8acd-Paper.pdf" target=&ldquo;blank&rdquo;>this paper</a>.
</p>
</li>
</ul>
<p>Bandits are actively being studied in both <a href="https://tor-lattimore.com/downloads/book/book.pdf" target=&ldquo;blank&rdquo;>theory</a> and <a href="http://rob.schapire.net/papers/www10.pdf" target=&ldquo;blank&rdquo;>applications</a> including <a href="https://mwtds.azurewebsites.net/" target=&ldquo;blank&rdquo;>deployable web service</a> and <a href="https://arxiv.org/abs/1603.06560" target=&ldquo;blank&rdquo;>hyperparameter optimization</a> (check <a href="https://docs.ray.io/en/master/tune/api_docs/schedulers.html#tune-scheduler-hyperband" target=&ldquo;blank&rdquo;>ray implementation</a>).
Also, the cartoon caption contest of New Yorker is using bandit algorithms to efficiently crowdsource caption evaluations (<a href="http://nextml.org/2015/12/04/new-yorker.html" target=&ldquo;blank&rdquo;>this article</a>)!
</p>
<h2>Service</h2>
<ul>
<li><p>Area chair: COLT, ALT, NeurIPS, AAAI, ICML
</p>
</li>
<li><p>Program committee: AISTATS
</p>
</li>
<li><p>Action editor for <i>Machine Learning</i>.
</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2025-01-28 09:12:33 MST, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
